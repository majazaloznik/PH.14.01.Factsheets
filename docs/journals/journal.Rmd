---
title: "Journal"
output:
  pdf_document: default
---
# Tuesday 5.12.2017

1. This project had been started over a month ago, and got pretty far, then was asked by SH to stop working on the FS.. Anyway, I'm now starting again, first by migrating it into my new project template, makefile and all.

This is what my makefile looks like graphically at the minute:

```{r, echo = FALSE, out.width='100%'}
knitr::include_graphics(paste0(rprojroot::find_root("Makefile"),"/figures/make.png"))
```

2. Initialise readme. And make it have the makefile plot. And the builiding of the readme be in the makefile plot. So meta ;)

3. Initial commit. 

4. Migration:

* DHS application form -> dictionarries

5. Now think through how to deal with the data import, because there is like 2GB of this stuff zipped, let alone unzipped. OK, there seems to be a library that helps with this, called `lodown` and this is part of the **Analyse survey data for free project** by Anthony Joseph Damico [http://asdfree.com/prerequisites.html]. So let's try that first. 

6. Requires new Rtools, then devtools package and finally `install_github( "ajdamico/lodown" , dependencies = TRUE )` to install the lodown package from github. 

7. OK, that didn't work :( Got " length(project.line) == 1 is not TRUE" error when tried to run `get_catalog()`, no idea why. 

8. OK, second try. So I start with a form on the DHS website, where i pick the countires, (all), the survey (individual recode) and the file type (stata system file). Then I get a url list. It seems that if I am logged in and in the correct 'project' in a browser, I can download from that browser. So presumably I won't be able to download fro R direct using `download.file()`. 

9. Right, what I'll do is simply download all the files into data/raw, period. Here is the process:

* Get the text file with the urls from the DHS website. saved in `data\raw\url.list.txt`.
* download all the (currenlty) 321 zip files into `data\raw`. Shit, I've got 318 of them already, which ones are missing?
* WOW, so turns out that the 'unique file naming system' is actually not unique at all, since the Indian surveys are actually split into states and (at least) one of these has the same name as Kenya `KEIR42DT.zip`. FFS. OK, so now I have to rename the Kenya ones as KN. MANUALLY. 
* Then prepare a dataframe - catalog - to deal with the importers etc.
* learn regular expressions to get out the survey ID, which helps distinguish the Indian ones. 

[DHS DOCUMENTATION LINK](https://www.dhsprogram.com/publications/publication-search.cfm?type=35)


# Wednesday 6.12.2017

1. OK, continuing with the `00-import.R` file, which should output individual `.rds` files, and a table summarising the datasets. Input is the url list (and the files already downloaded in the `/data/raw` folder. 

2. This is the annoying manual shit: I have to fix the KE file extensions to KN, in the folder, and in the catalog dataframe. 

3. During importation there are numerous warning messages for duplicate labels. We are summarily ignoring these. 

4. Preparing a funciton to import and clean up after itself.. A bit complicated ;) The `Stata.file()` function - the importer - unzipps the file into the working dir, i need to get rid of the files.. Also the .dta files have different names, i.e. cases..

5. Let it run over night.

# Thursday 7.12.2017

1. Stopped over night because there was not enough disk space.. I couldn't automatically delete the .dta files, because the importer is some sort of external pointer that is kept open randomly and makes it impossible to delete the file, even though you then can a while later or sth.. Anyway.

2. Reran the rest of the files. It did get stuck again a little bit with some dta error - version 0 isn't supported, but that cleared up magically after I reran teh third time. 

3. Looks like the last four or so importers keep an open connection.

4. Error in get.dictionary.dta(dta) : version 80 not yet supported, happened again. 

# Tuesday 12.12.17

1. Got word from the `lodown` package developer that I might want to look at the project name I have on the DHS website, and it looks like the length of the name made lodown not work. So actually if I change it to sth shorter it would work. [[https://github.com/ajdamico/lodown/issues/124]]

2. This is very annoying mainly because I've already done the whole thing manually in the meantime. Classic. 

# Thursay 14.12.17

1. Argh, for complete redproducibility let's go and do the `lodown` thing then. 

2. What Anthony, the `lodown` guy might find useful is a regex thing to maket he catalogue a bit more usefil, because the catalog doesn't distinguish different types of files (surveys and filetypes). 

3. Also, the lodown catalog for DHS puts every file in it's own folder it seems, so let's first try to make it not do that.

4. Right, seems that I extracted the dataset type and filetypes correctly, as i end up with 321 files in my catalog, same as I had in the manually ticked url list from the DHS website.

5. Now change the download folder. Test with single dataset. 

6. OK, testing looks good! It's downloading the file, and unzipping it, and looks like it's also savign the .dta file as an .rds object. 

# Friday 15.12.17

1. Now need to get the `00-import.R` to source properly from the makefile. 

2. So how can I stop the `00-import.R` script from importing the catalog every time? Ah, nice, separate the catalog build from the import. Now won't have to repeat it every time. 

3. Now I have a different problem. The lodown package is great in extracting the file, but it reads the `dta` file and saves it as a dataframe, thereby loosing the value label data. 

4. AAh, but it isn't, it uses `haven::read_dta`, so the labels are there, if only in some weird `labelling` format.. But OK, you have to use `haven::as_facto()` to get it to behave as a factor, all good. 

5. Now, let's try and run the whole download thing, that should be the last of 01-import.R And let me commit this now as working download.. 

* ToDO suppressPackageStartupMessages()